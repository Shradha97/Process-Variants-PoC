{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Variants\n",
    "\n",
    "\n",
    "\n",
    "| rule    | variant_ID | tasks_list |     variant_name       | count | avg_TAT | avg_PT |\n",
    "| ------- | :---------:|:----------:|:----------------------:|:-----:|:-------:|:------:|\n",
    "| Default |     1      |  [T1,T2]   |     Variant T1 T2      |   2   |   20    |   10   |\n",
    "| Default |     2      | [T1,T2,T4] |     Variant T1 T2 T4   |   1   |   40    |   20   |\n",
    "| Default |     2      | [T2,T1,T4] |     Variant T1 T2 T4   |   1   |   50    |   11   |\n",
    "| Default |     3      | [T1,T2,T3] |     Variant T1 T2 T3   |   2   |   10    |    9   |\n",
    "| Default |     3      | [T2,T1,T3] |     Variant T1 T2 T3   |   1   |   30    |   17   |\n",
    "\n",
    "**Notes**\n",
    "* Each variant set is based on some Rule. \n",
    "* For now \"default\" rule is first occurance of task instance defines the order of tasks in the variant\n",
    "* Variant ID is unique for each variant set within a rule\n",
    "* Tasks are ordered based on the order of their occurance in the variant\n",
    "* Variant name is the name of the variant set. If possible it could be different for sub variants\n",
    "* we may use LLM for generating variant names\n",
    "* count is the number of cases in the variant set\n",
    "* avg_TAT is the average TAT of the cases in the variant set\n",
    "* avg_PT is the average Processing time of the cases in the variant set \n",
    "* task -> defined or not can vary depending on the rule -> can define when a task is complete and only complete tasks appear in the task_list for the variant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI, AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from configs.config import cache as cache_path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "in_file_name = \"features.csv\"\n",
    "out_file_name = \"variants.csv\"\n",
    "variants_name_file = \"variants_names.json\"\n",
    "out_path = cache_path + out_file_name\n",
    "features_data = pd.read_csv(cache_path + in_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_variant_name(data):\n",
    "    # Converting task list to set and assigning variant names directly\n",
    "    data['variant_name'] = 'variant ' + data['task_list'].apply(lambda x: ' '.join(sorted(set(x))))\n",
    "\n",
    "    # change task list from tuple to list\n",
    "    data['task_list'] = data['task_list'].apply(list)\n",
    "\n",
    "    return data\n",
    "\n",
    "def aggregate_unique_variants_count(data):\n",
    "    # Define aggregation functions\n",
    "    agg_functions = {'task': ['size'], 'case_TAT': ['mean'], 'processing_time': ['mean']}\n",
    "\n",
    "    # Aggregate by grouping on 'TASK' and 'VARIANT_ID', calculating count, mean of 'CASE_TAT', and 'VARIANT_ID'\n",
    "    aggregrated_counts = data.groupby(['variant_ID', 'task']).agg(agg_functions).reset_index()\n",
    "\n",
    "    # Rename columns\n",
    "    aggregrated_counts.columns = ['variant_ID', 'task_list', 'count', 'avg_TAT', 'avg_PT']\n",
    "\n",
    "    return aggregrated_counts\n",
    "\n",
    "def get_variant_ID(data, rule):\n",
    "    # Generate unique IDs based on the set of tasks in 'task_list'\n",
    "    task_list_ids = data['task'].apply(lambda x: hash(tuple(sorted(x)))).astype(str)\n",
    "    data['variant_ID'] = pd.factorize(task_list_ids)[0] + 1\n",
    "\n",
    "    return data\n",
    "\n",
    "def aggregate_task_as_list(data):\n",
    "    # Convert 'FIRST_TS' and 'LAST_TS' columns to datetime\n",
    "    data[['first_TS', 'last_TS']] = data[['first_TS', 'last_TS']].apply(pd.to_datetime)\n",
    "\n",
    "\n",
    "    # Sort the data by 'FIRST_TS' within each case, then group tasks by 'CASE' and aggregate them into a list\n",
    "    aggregated_tasks = data.sort_values(by=['case', 'first_TS']).groupby('case').agg({\n",
    "        'task': tuple,\n",
    "        'processing_time': 'sum'  # Calculate the sum of processing times for each case\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate 'CASE_TAT' column as the difference between the maximum last timestamp and minimum first timestamp of the case\n",
    "    # UPDATE THIS!!!\n",
    "    aggregated_tasks['case_TAT'] = data.groupby('case').apply(lambda x: (x['last_TS'].max() - x['first_TS'].min()).total_seconds()).values\n",
    "\n",
    "    # drop the CASE column\n",
    "    aggregated_tasks = aggregated_tasks.drop('case', axis=1)\n",
    "\n",
    "    return aggregated_tasks\n",
    "\n",
    "def create_variants_table(rule, features_data):\n",
    "    # Aggregate tasks as a list\n",
    "    aggregated_tasks = aggregate_task_as_list(features_data)\n",
    "\n",
    "    # Create unique IDs for each unique task list\n",
    "    aggregated_tasks = get_variant_ID(aggregated_tasks, rule)\n",
    "\n",
    "    # Aggregate unique variants count\n",
    "    aggregated_tasks = aggregate_unique_variants_count(aggregated_tasks)\n",
    "\n",
    "    # Add variant name\n",
    "    aggregated_tasks = add_variant_name(aggregated_tasks)\n",
    "\n",
    "    # add a rule column as the first column\n",
    "    aggregated_tasks.insert(0, 'rule', rule)\n",
    "\n",
    "    return aggregated_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create variants according to a rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rule(rule = 'default'):\n",
    "    return rule\n",
    "\n",
    "rule = get_rule()\n",
    "variants_data = create_variants_table(rule, features_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Variant Names using ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ChatGPT API creds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_endpoint = os.getenv(\"AZURE_OPENAI_GPT4_ENDPOINT\") # os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_GPT4_KEY\") # os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "api_type = 'azure'\n",
    "model_deployment_name=os.getenv(\"AZURE_OPENAI_GPT4_DNAME\") # os.getenv(\"AZURE_OPENAI_DNAME\")\n",
    "model_api_version='2023-05-15'\n",
    "max_tokens = 1000\n",
    "chat_temperature = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for calling API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_template():\n",
    "    variant_name_generation_prompt_template = \"\"\"\n",
    "    In the variants_list delimited by <>, a list of variant names is given.\n",
    "    Each variant name is a sequence of task names separated by a whitespace present in that variant. \n",
    "    Do the following:\n",
    "    1. For each variant name, generate a meaningful name with max 6 words for the variant, ignore the task ordering, \n",
    "    generate the most meaningful name.\n",
    "    2. Place each of this new variant name in the variants_list in the same order as the original variant names.\n",
    "    3. Return a python dictionary with the key as the original variant name and the value as the new variant name.\n",
    "    4. Omit any kind of other information or explanation in the output.\n",
    "\n",
    "    variants_list: <{variants_list}>\n",
    "    \"\"\"\n",
    "    return variant_name_generation_prompt_template\n",
    "\n",
    "def set_llm(prompt_input_variable='variants_list'):\n",
    "    prompt = PromptTemplate(\n",
    "        template=get_prompt_template(), input_variables=[prompt_input_variable], output_parser=CommaSeparatedListOutputParser()\n",
    "    )\n",
    "\n",
    "    llm_chain = AzureChatOpenAI(\n",
    "        deployment_name=model_deployment_name,\n",
    "        openai_api_version=model_api_version,\n",
    "        azure_endpoint=api_endpoint,\n",
    "        openai_api_key=api_key,\n",
    "        temperature=chat_temperature,\n",
    "        streaming=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm_chain)\n",
    "\n",
    "    return llm_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random names and send back the name to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_variant_names(llm_chain, variant_names, batch_size=15):\n",
    "    list_dict = {}\n",
    "    batch_number = 0\n",
    "\n",
    "    # Split variant_names into batches using list comprehension\n",
    "    batches = [variant_names[i:i + batch_size] for i in range(0, len(variant_names), batch_size)]\n",
    "\n",
    "    print(f\"Total batches to process: {len(batches)}\")\n",
    "\n",
    "    # Process batches and update list_dict\n",
    "    for batch in batches:\n",
    "        print(f\"Processing batch {batch_number}...\")\n",
    "        result = eval(llm_chain.predict(variants_list=batch))\n",
    "        \n",
    "        # Update list_dict with results, using batch elements as keys\n",
    "        list_dict.update(result)\n",
    "        batch_number += 1\n",
    "\n",
    "    return list_dict\n",
    "    \n",
    "\n",
    "def replace_variant_names(llm_chain, variant_names, new_variants_names):\n",
    "    variant_names_list = variant_names.values.tolist()\n",
    "\n",
    "    if new_variants_names:\n",
    "        print(\"Generated variant names found, reading names from file\")\n",
    "        if set(variant_names_list) == set(new_variants_names.keys()):\n",
    "            mapped_variants = list(map(new_variants_names.get, variant_names_list))\n",
    "\n",
    "            return mapped_variants\n",
    "        else:\n",
    "            print(\"Variant names mismatch, generating new variants names\")\n",
    "            new_variant_names = generate_variant_names(llm_chain, variant_names_list)\n",
    "            mapped_variants = list(map(new_variant_names.get, variant_names_list))\n",
    "            with open(cache_path + variants_name_file, 'w') as outfile:\n",
    "                json.dump(new_variant_names, outfile)\n",
    "\n",
    "            return mapped_variants\n",
    "    else:\n",
    "        print(\"Generated variant names not found, generating new variants names\")\n",
    "        new_variant_names = generate_variant_names(llm_chain, variant_names_list)\n",
    "        mapped_variants = list(map(new_variant_names.get, variant_names_list))\n",
    "        with open(cache_path + variants_name_file, 'w') as outfile:\n",
    "            json.dump(new_variant_names, outfile)\n",
    "\n",
    "        return mapped_variants\n",
    "\n",
    "\n",
    "def run_variant_names_generation_pipeline(variants_data, variants_id, use_chatgpt=False):\n",
    "    if not use_chatgpt:\n",
    "        new_variant_names = \"variant \" + variants_id.astype(str)\n",
    "        mapped_variants = {k: v for k, v in zip(variants_data, new_variant_names)}\n",
    "        with open(cache_path + variants_name_file, 'w') as outfile:\n",
    "            json.dump(mapped_variants, outfile)\n",
    "        return new_variant_names\n",
    "    \n",
    "    variants_names = None\n",
    "    # read the json file with the variant names if it exists\n",
    "    if os.path.exists(cache_path + variants_name_file):\n",
    "        with open(cache_path + variants_name_file) as json_file:\n",
    "            variants_names = json.load(json_file)\n",
    "\n",
    "    llm_chain = set_llm()\n",
    "    return replace_variant_names(llm_chain, variants_data, variants_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the generation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_data.rename(columns={'variant_name': 'old_variant_name'}, inplace=True)\n",
    "variants_data['variant_name'] = run_variant_names_generation_pipeline(variants_data['old_variant_name'], variants_data['variant_ID'], use_chatgpt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export variants to variants.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the variants data to csv\n",
    "variants_data.to_csv(cache_path + out_file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
